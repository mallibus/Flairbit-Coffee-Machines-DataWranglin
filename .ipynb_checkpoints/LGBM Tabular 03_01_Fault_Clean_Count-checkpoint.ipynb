{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset filename .csv decoding\n",
    "\n",
    "e.g. **Faults_Cleaning_Counters_100_D_20_1.csv**\n",
    "\n",
    "Field | Description\n",
    "------|-----------------\n",
    "Faults_Cleaning_Counters | what is in the datset\n",
    "100 |  number of machines Serial Numbers in the datset. \n",
    "D | time period of summarization D=Days W=Weeks\n",
    "10 | depth of each row in periods\n",
    "1 | time periods between two consecutive rows for the same Serial Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"Faults_Cleaning_Counters_100_D_20_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(dataset_root+\"_0000.csv\",parse_dates = ['Target Timestamp']),\n",
    "                pd.read_csv(dataset_root+\"_0001.csv\",parse_dates = ['Target Timestamp']),\n",
    "                pd.read_csv(dataset_root+\"_0002.csv\",parse_dates = ['Target Timestamp']),\n",
    "                pd.read_csv(dataset_root+\"_0003.csv\",parse_dates = ['Target Timestamp'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62300 entries, 0 to 15574\n",
      "Columns: 605 entries, Unnamed: 0 to TARGET\n",
      "dtypes: datetime64[ns](1), float64(601), int64(2), object(1)\n",
      "memory usage: 288.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset rearranging\n",
    "\n",
    "The dataset comes with the TARGET column being the number of faults in the target day. I can make the prediction more general (and more useful) builind a target as the occurrence of a fault in one of the last N days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the preprocessing general I get the information about the categories and the time period from the data itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['numacqua', 'numcaffegenerale', 'ngr1', 'ngr4', 'numcaffegr3', 'nummac2gr1', 'numcaffegr1', 'portatagr1', 'tempogr4', 'numcaffegr2', 'ngr2', 'numcioccolato', 'tempogr2', 'numsolubile', 'ngr3', 'numcicligr1', 'numlattefr', 'tempogr1', 'numlattegr1', 'nummac1gr1', 'FAULT', 'portatagr2', 'CLEANING', 'numvapore', 'tempogr3', 'numvaporeariats', 'portatagr4', 'numcaffegr4', 'numvaporets', 'portatagr3']\n"
     ]
    }
   ],
   "source": [
    "#List of root names of categories\n",
    "#For each column name, get the ones with a '-' in the name, take the part of the names before the '-' and make it a set.\n",
    "categories_roots = list(set([s.split('-')[0] for s in df.columns if '-' in s]))\n",
    "print(categories_roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n"
     ]
    }
   ],
   "source": [
    "#List of root names of categories\n",
    "#For each column name, get the ones with a '-' in the name, take the part of the names After the '-' and make it a set.\n",
    "time_period=list(set([s.split('-')[1][-1] for s in df.columns if '-' in s]))[0]\n",
    "print(time_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamp column is in pd.Timestamp type - transform it in a cardinal number starting from the first date of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target Timestamp'].min()\n",
    "df['Target day'] = (df['Target Timestamp']-df['Target Timestamp'].min()).apply(lambda d: d.days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction width parameter\n",
    "\n",
    "How many days to aggregate to build the TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target of the prediction is the aggregation of N days\n",
    "target_width = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FAULT-4D', 'FAULT-3D', 'FAULT-2D', 'FAULT-1D', 'TARGET']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_sum = ['FAULT-%d%s'%(n,time_period) for n in range(target_width-1,0,-1)]\n",
    "columns_to_sum.append('TARGET')\n",
    "columns_to_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to drop the columns relevant to the period I want to predict - all type of columns, not just faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "columns_to_drop = [['%s-%d%s'%(root,n,time_period) for n in range(target_width-1,0,-1)] for root in categories_roots]\n",
    "columns_to_drop = list(chain(*columns_to_drop))\n",
    "#columns_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Build the new TARGET column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['TARGET']=df[columns_to_sum].sum(axis=1)\n",
    "# For now only boolean prediction\n",
    "df['TARGET'] = (df['TARGET']>0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    40929\n",
       "1    21371\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Serial', 'Model', 'Target Timestamp', 'CLEANING-20D',\n",
      "       'CLEANING-19D', 'CLEANING-18D', 'CLEANING-17D', 'CLEANING-16D',\n",
      "       'CLEANING-15D',\n",
      "       ...\n",
      "       'tempogr4-8D', 'tempogr4-7D', 'tempogr4-6D', 'tempogr4-5D',\n",
      "       'tempogr4-4D', 'tempogr4-3D', 'tempogr4-2D', 'tempogr4-1D', 'TARGET',\n",
      "       'Target day'],\n",
      "      dtype='object', length=606)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unwanted columns either because part of the target days or because useless (Serial, Timestamp, Unnamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop.extend(['Unnamed: 0','Target Timestamp','Serial'])\n",
    "df = df.drop(columns_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the \"Model\" column Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Model'] = df['Model'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import Machine Learning stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from datetime import datetime\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light GBM classifier is used to allow eventually the uasge of Grid Search for params tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lgbm_classifier to use Gridsearch\n",
    "def lgbm_classifer(df,learning_rate = 0.1, num_leaves = 31,n_estimators=100,num_iterations=100,verbose=False):\n",
    "\n",
    "    X = df.drop('TARGET',axis=1)\n",
    "    y = df['TARGET'].astype(int)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)\n",
    "\n",
    "    lgbmc = lgb.LGBMClassifier(num_leaves     = num_leaves,\n",
    "                               n_estimators   = n_estimators,\n",
    "                               learning_rate  = learning_rate,\n",
    "                               num_iterations = num_iterations,\n",
    "                               objective      = 'binary',\n",
    "                               boosting_type  = 'gbdt' ,\n",
    "                               is_unbalance   = True,\n",
    "                               metric         = ['binary_logloss'])\n",
    "\n",
    "    start=timeit.default_timer()\n",
    "    lgbmc.fit(X_train,y_train,\n",
    "                   #categorical_feature =[0],\n",
    "                   feature_name = list(X.columns))\n",
    "    stop=timeit.default_timer()\n",
    "    \n",
    "    prob_pred = lgbmc.predict(X_test)\n",
    "    predictions = (prob_pred > 0.5).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_test,predictions)\n",
    "    f1 = f1_score(y_test,predictions)\n",
    "    if verbose:\n",
    "        print(\"Fitting time: {} s\".format(stop-start))\n",
    "        print(\"Classification Report\")\n",
    "        print(classification_report(y_test,predictions))\n",
    "        print(\"Confusion Matrix\")\n",
    "        print(cm)\n",
    "        print('F1:',f1)\n",
    "\n",
    "    feature_importance = pd.DataFrame({'f_name':list(X.columns),'importance':lgbmc.feature_importances_}).dropna()\n",
    "    feature_importance = feature_importance[feature_importance['importance']>0].\\\n",
    "            sort_values('importance',ascending=False).set_index('f_name')['importance'][:30]\n",
    "\n",
    "    return(f1,feature_importance,lgbmc)   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time: 11.119897696969701 s\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91     13397\n",
      "           1       0.82      0.86      0.84      7162\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     20559\n",
      "   macro avg       0.87      0.88      0.88     20559\n",
      "weighted avg       0.89      0.89      0.89     20559\n",
      "\n",
      "Confusion Matrix\n",
      "[[12095  1302]\n",
      " [ 1025  6137]]\n",
      "F1: 0.840627354290802\n",
      "F1: 0.840627354290802\n",
      "f_name\n",
      "FAULT-5D                559\n",
      "FAULT-7D                538\n",
      "FAULT-6D                533\n",
      "Model                   471\n",
      "FAULT-11D               469\n",
      "FAULT-17D               448\n",
      "FAULT-20D               433\n",
      "FAULT-18D               429\n",
      "FAULT-10D               409\n",
      "FAULT-8D                393\n",
      "numcaffegenerale-5D     392\n",
      "FAULT-9D                390\n",
      "numcaffegenerale-8D     382\n",
      "FAULT-16D               375\n",
      "FAULT-15D               362\n",
      "numacqua-15D            357\n",
      "numcaffegenerale-19D    352\n",
      "numcaffegr1-17D         352\n",
      "numcaffegenerale-12D    345\n",
      "numcaffegenerale-9D     333\n",
      "numcaffegenerale-20D    327\n",
      "numcaffegr1-6D          326\n",
      "numcaffegenerale-6D     324\n",
      "numcaffegr1-20D         318\n",
      "numcaffegenerale-17D    317\n",
      "numcaffegenerale-15D    312\n",
      "numacqua-20D            303\n",
      "numcaffegenerale-7D     302\n",
      "numcaffegenerale-13D    298\n",
      "numcaffegenerale-18D    297\n",
      "Name: importance, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "f1,ranks,classifier = lgbm_classifer(df.drop('Target day',axis=1),num_leaves  = 350,\n",
    "                           n_estimators   = 100,\n",
    "                           learning_rate  = 0.01,\n",
    "                           num_iterations = 100,\n",
    "                           verbose=True)\n",
    "\n",
    "print(\"F1:\",f1)\n",
    "print(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
